{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cat-Dog Classifier"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "By Mathew Kouch.\n",
    "\n",
    "A ResNet18 based model to distinguish cat pics from dog pics. \n",
    "\n",
    "A binary classifier trained via transfer learning of ResNet18 on the cats-vs-dogs Kaggle dataset of 12.5K cats and 12.5K dogs photos with accuracy of 95%.\n",
    "\n",
    "Details of training:\n",
    "    batch_size = 128\n",
    "    transforms = Resize(224,224), Normalisation\n",
    "    EPOCHS = 1\n",
    "    lr = 0.001\n",
    "    optimizer = Adam\n",
    "    criterion = CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import torch.nn as nn # For making NN layers\n",
    "import torch.nn.functional as F # For activation functions\n",
    "import torch.optim as optim # For optimizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd # To creaste cvs submission file\n",
    "\n",
    "import torchvision # library for handling images\n",
    "from torchvision import datasets, transforms, models # for making datasets, preprocessing data, pretrained models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "\n",
    "import torchvision.models as models # to get pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU\n"
     ]
    }
   ],
   "source": [
    "# Setting torch variables to GPU else cpu for training\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    torch.device('cuda')\n",
    "    device = 'cuda'\n",
    "    print('Training on GPU')\n",
    "else:\n",
    "    torch.device('cpu')\n",
    "    print('Training on CPU')\n",
    "    \n",
    "# Sets batchsize of training images\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Custom Datasets Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DatasetProcessing(Dataset):\n",
    "    '''Dataset is created via reading image in image directory. Images titles are labelled with an id e.g cat.2022.jpg'''\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = annotations_file\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # the image name for reading the image at idx\n",
    "        img_name = 'cat'+'.'+str(idx)+'.jpg'\n",
    "        if idx>=12500:\n",
    "            img_name = 'dog'+'.'+str(idx%12500)+'.jpg'\n",
    "        \n",
    "        # directory of image at idx\n",
    "        img_path = os.path.join(self.img_dir,img_name)\n",
    "        # load image at idx and normalise\n",
    "        image = read_image(img_path)/255\n",
    "        # label of image at idx\n",
    "        label = self.img_labels[idx]\n",
    "        # transform image and its label\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        \n",
    "        # returns image and label tuple\n",
    "        return image, label\n",
    "    \n",
    "class TestDatasetProcessing(Dataset):\n",
    "    '''The test data is created by reading test images from their directory.'''\n",
    "    def __init__(self,img_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return 12500\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # the image name for reading the image at idx\n",
    "        img_name = str(idx+1)+'.jpg'\n",
    "        # directory of image at idx\n",
    "        img_path = os.path.join(self.img_dir,img_name)\n",
    "        # load image at idx and normalise\n",
    "        image = read_image(img_path)/255\n",
    "       \n",
    "        # transform image and its label\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        # returns image and label tuple\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cats are 0 for first 12500\n",
    "labels = np.zeros((25000))\n",
    "# Dogs are the last 12500\n",
    "labels[12500:]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Training Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size=(224,224)),\n",
    "    ])\n",
    "target_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "# Datasets and DatLoader.\n",
    "# ! Change the image folder path !\n",
    "train_data =  DatasetProcessing(labels,'dataset/train', transform = transform, target_transform=None)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Func."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(predictions, labels):\n",
    "    '''\n",
    "    Calculates the predictions of the NN by counting \n",
    "    number of prediction correct over total predictions\n",
    "    '''\n",
    "    return predictions.argmax(dim=1).eq(labels).sum().item()\n",
    "\n",
    "def train(model, device, dataloader, optimizer, criterion,epoch):\n",
    "    '''The training loop used to train the model'''\n",
    "    # Records the loss and accuracy for every epoch (one passthrough of dataloader)\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (images,labels) in enumerate(dataloader):\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # outputs a 1x2 tensor of 1dim tensor scores\n",
    "        pred = model(images) \n",
    "        \n",
    "        # Cross Entropy loss between predicted and actual\n",
    "        loss = criterion(pred,labels)\n",
    "        \n",
    "        # Zero all gradients in Computational Graph\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Perform Back Propagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform gradient descent\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Count correct predictions\n",
    "        with torch.no_grad():\n",
    "            acc = get_accuracy(pred,labels)\n",
    "        \n",
    "        # Accumulating training loss and accuracy throughout one epoch\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc\n",
    "        \n",
    "        # Print metrics per interval\n",
    "        if batch_idx%25==0:\n",
    "            print(f'| Epoch: {epoch+1} | Acc: {100*acc/len(labels):5.2f}% | Loss: {epoch_loss/(batch_idx+1):.3f}|')\n",
    "    \n",
    "    # Returns metric\n",
    "    return epoch_loss / 25000, epoch_acc / 25000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ResNet18 as a feature extractor. WIll remove last 1000 class classifier with only a binary classifier for cats and dogs. Then will fine-tune on training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Resnet18 on ImageNet predicts 1000 classes\n",
    "model = models.resnet18(pretrained=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing all features of the ResNet18 feature extractor\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new cat-dog 2 nuerons F.C linear layer binary classifier\n",
    "model.fc = nn.Linear(in_features=512, out_features=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criterion\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer \n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: torch.Size([128, 3, 224, 224])\n",
      "| Epoch: 1 | Acc: 37.50% | Loss: 0.850|\n",
      "Pred: torch.Size([128, 3, 224, 224])\n",
      "| Epoch: 1 | Acc: 96.88% | Loss: 0.220|\n",
      "| EPOCH: 1 | Train Loss: 0.001 | Train Acc: 94.12% |\n"
     ]
    }
   ],
   "source": [
    "# One epoch is enough for high 90s percent accuracy and generalisation.\n",
    "EPOCHS = 1 \n",
    "\n",
    "# Model is saved during training based on highest training accuracy\n",
    "SAVE_DIR = 'models'\n",
    "MODEL_SAVE_PATH = os.path.join(SAVE_DIR, 'resnet18-cat-dog-calssifier-1-epoch-v3.pt')\n",
    "\n",
    "best_train_acc = 0\n",
    "\n",
    "# Training loss and acc logger to plot performance\n",
    "train_loss_logger = []\n",
    "train_acc_logger = []\n",
    "\n",
    "# Checks if previous models have been saved in folder, else make one.\n",
    "if not os.path.isdir(f'{SAVE_DIR}'):\n",
    "    os.makedirs(f'{SAVE_DIR}')\n",
    "\n",
    "# Finally, the training begins...\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, device, train_loader, optimizer, criterion, epoch)\n",
    "    \n",
    "    # Recording loss and acc per epoch\n",
    "    train_loss_logger.append(train_loss)\n",
    "    train_acc_logger.append(train_acc)\n",
    "    \n",
    "    # Save model with highest accuracy\n",
    "    if train_acc > best_train_acc:\n",
    "        best_train_acc = train_acc\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    \n",
    "    print(f'| EPOCH: {epoch+1} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:5.2f}% |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9412"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.00125]), [0.9412])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(train_loss_logger,5), train_acc_logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just some test code to check validation of model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test image file path\n",
    "img_dir = 'dataset/test/12228.jpg'\n",
    "# Reading image as tensor\n",
    "cat1 = read_image(img_dir)\n",
    "cat1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a transform function to resize image to 224,224 thats required for ResNet18\n",
    "transform_resize = torchvision.transforms.Resize((224,224))\n",
    "\n",
    "# Normalisation of image as thats what is required...\n",
    "cat1 = transform_resize(cat1)/255\n",
    "\n",
    "# Converting image to FloatTensor as required by ResNet18\n",
    "cat1 = cat1.type(torch.FloatTensor).unsqueeze(0)\n",
    "\n",
    "# Checking input image shape is 3 X 224 X 224, and type tensor as required for ResNet18\n",
    "cat1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dog'"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set model to eval mode because we dont wont \n",
    "# drop out layers etc to happen during inference\n",
    "model.eval()\n",
    "\n",
    "# Inputing the image and getting back logit prediction as numpy, \n",
    "# now in cpu not gpu so we can use np.argmax() on it....\n",
    "score = model(cat1.to(device)).detach().cpu().numpy()\n",
    "\n",
    "# FINALLY, the class prediction...\n",
    "p = 'Cat' if np.argmax(score[0])==0 else 'Dog'\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list to hold all the predictions\n",
    "pred = []\n",
    "\n",
    "# Creating the test dataset for DataLoader\n",
    "test_dataset = TestDatasetProcessing('dataset/test',transform=transform)\n",
    "\n",
    "# Creating the DataLoader from dataset so NN can iterate over a batch... \n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, the test inference.\n",
    "# Making sure model in evlaution mode \n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    \n",
    "    # Iterate over batch image in data loader, perform inference per batch\n",
    "    for batch, image in enumerate(test_loader):\n",
    "        # Make logit scores\n",
    "        score = model(image.to(device)).detach().cpu().numpy()\n",
    "        \n",
    "        # Make predictions based off logits\n",
    "        pred_class = 0 if np.argmax(score[0])==0 else 1\n",
    "        \n",
    "        # Append predictions to pred list for csv submission\n",
    "        pred.append(pred_class)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing predictions to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pandas dataframe with id and label header.\n",
    "# Will convert this to csv.\n",
    "output = pd.DataFrame({'id':np.arange(1,12501), 'label':pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label\n",
       "0   1      1\n",
       "1   2      1\n",
       "2   3      1\n",
       "3   4      1\n",
       "4   5      0"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the that id and predictions are in correct dtypes.\n",
    "# e.g labels are in integers not float, and id starts at 1 not 0\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat-Dog predictions saved! :)\n"
     ]
    }
   ],
   "source": [
    "# Convert pandas dataframe to csv\n",
    "output.to_csv(f'submission.csv', index=False)\n",
    "\n",
    "# Print something motivating, as all the work is finally done!\n",
    "print('CONGRATULATIONS! Cat-Dog predictions saved! :)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
